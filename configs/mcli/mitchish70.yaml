name: olmo-70b
image: mosaicml/pytorch:2.1.2_cu121-python3.10-ubuntu20.04
compute:
  cluster: r14z3p2
  gpus: 256
  gpu_type: h100_80gb
integrations:
  - integration_type: git_repo
    git_repo: allenai/OLMo
    git_branch: mitchish65-2-gqa
    pip_install: -e .
    ssh_clone: true
command: |-
  # Make sure we have a recent flash-attn.
  # NOTE: only pinning flash-attn here to future proof it.
  pip install flash-attn==2.5.3 --no-build-isolation

  # Show packages for debugging.
  pip freeze

  # Prepare environment.
  cd OLMo
  mkdir -p /root/.cache/torch
  export OMP_NUM_THREADS=8
  export LOG_FILTER_TYPE=local_rank0_only

  torchrun \
  --master_addr "$MASTER_ADDR" \
  --master_port "$MASTER_PORT" \
  --nnodes "$NUM_NODES" \
  --node_rank "$NODE_RANK" \
  --nproc_per_node 8 \
  scripts/train.py configs/mitchish70-s3.yaml \
    --run_name=mitchish70-profiled \
    --model.flash_attention=true \
    --save_interval_ephemeral=100 \
    --stop_at=200 \
    --device_train_microbatch_size=4 \
    --torch_profiling=true \
    --evaluators=[] \
    --wandb.group=mitchish70-test
