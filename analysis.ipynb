{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a47350",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import entropy as compute_entropy\n",
    "import json\n",
    "import argparse\n",
    "import scipy.stats\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import OlmoeForCausalLM, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import time\n",
    "import pickle as pkl\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c365788",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(\"/home/mila/k/khandela/scratch/ai2-llm/checkpoints/OLMoE/multilingual-5langs-lbl-1500\",0)\n",
    "layers_to_analyze = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "languages = [\"en\", \"hi\", \"ar\", \"ru\", \"zh\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdecdbc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "all_lang_expert_counts = defaultdict(lambda: defaultdict(Counter))\n",
    "all_lang_token_assignments = defaultdict(lambda: defaultdict(lambda: defaultdict(Counter)))\n",
    "position_based_counts_all = defaultdict(lambda: defaultdict(lambda: defaultdict(Counter)))\n",
    "coactivation_counts_all = defaultdict(lambda: defaultdict(Counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a5706d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for lang in languages:\n",
    "    file_path = f\"/home/mila/k/khandela/OLMoE/scripts/data/validation/{lang}.txt\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read().replace(\"\\n\", \" \")\n",
    "\n",
    "    # Run the analysis for the current language\n",
    "    layer_counts, token_map = collect_language_expert_stats(text, layers_to_analyze, model, tokenizer)\n",
    "\n",
    "    for layer in layers_to_analyze:\n",
    "        all_lang_expert_counts[lang][layer].update(layer_counts[layer])\n",
    "        for expert_id, tokens in token_map[layer].items():\n",
    "            all_lang_token_assignments[lang][layer][expert_id].update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c7901",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "entropies, kls = plot_entropy_kl_summary(all_lang_expert_counts, layers_to_analyze)\n",
    "plot_kl_heatmaps(all_lang_expert_counts, layers_to_analyze,languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53657b4f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "get_shared_and_unique_experts(all_lang_expert_counts, layers_to_analyze, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c50ff77",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "get_shared_and_unique_experts_decoded(all_lang_expert_counts, all_lang_token_assignments, layers_to_analyze, tokenizer, 0.05, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70100903",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plot_all_langs_expert_token_counts(all_lang_expert_counts,layers_to_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676ea7df",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# model, tokenizer = load_model(\"/home/mila/k/khandela/scratch/ai2-llm/checkpoints/OLMoE/multilingual-5langs-nolbl-1900\",0)\n",
    "model, tokenizer = load_model(\"allenai/OLMoE-1B-7B-0924-Instruct\",0)\n",
    "layers_to_analyze = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "languages = [\"en\", \"hi\", \"ar\", \"ru\", \"zh\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf10dfc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "\n",
    "# Initialize data structures\n",
    "all_lang_expert_counts = defaultdict(lambda: defaultdict(Counter))\n",
    "all_lang_token_assignments = defaultdict(lambda: defaultdict(lambda: defaultdict(Counter)))\n",
    "position_based_counts_all = defaultdict(lambda: defaultdict(lambda: defaultdict(Counter)))\n",
    "coactivation_counts_all = defaultdict(lambda: defaultdict(Counter))\n",
    "\n",
    "\n",
    "jsonl_file_path = \"/home/mila/k/khandela/OLMoE/global_mmlu_multilingual_subjects.jsonl\"\n",
    "\n",
    "for lang in languages:\n",
    "    with open(jsonl_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            for category, translations in entry.items():\n",
    "                sentence = translations.get(lang)\n",
    "                if not sentence:\n",
    "                    continue\n",
    "                \n",
    "                layer_counts, token_map = collect_language_expert_stats(sentence, layers_to_analyze, model, tokenizer)\n",
    "\n",
    "                for layer in layers_to_analyze:\n",
    "                    all_lang_expert_counts[lang][layer].update(layer_counts[layer])\n",
    "                    for expert_id, tokens in token_map[layer].items():\n",
    "                        all_lang_token_assignments[lang][layer][expert_id].update(tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bbc8a1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plot_kl_heatmaps(all_lang_expert_counts, layers_to_analyze,languages)\n",
    "entropies, kls = plot_entropy_kl_summary(all_lang_expert_counts, layers_to_analyze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95538fa0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "# Example: Pre-assign colors to languages\n",
    "color_map = {\n",
    "    'en': 'tab:blue',\n",
    "    'hi': 'tab:orange',\n",
    "    'zh': 'tab:red',\n",
    "    'ar': 'tab:green',\n",
    "    'ru' : 'tab:purple',\n",
    "    # add more languages if needed\n",
    "}\n",
    "\n",
    "# (Optional) if you want to auto-assign colors dynamically\n",
    "# colors = cm.get_cmap('tab10', len(languages))\n",
    "# color_map = {lang: colors(i) for i, lang in enumerate(languages)}\n",
    "plt.figure(figsize=(10, 4))\n",
    "for lang in languages:\n",
    "    color = color_map.get(lang, 'black')\n",
    "    plt.plot(layers_to_analyze, entropies[lang], marker='o', label=lang.upper(), color=color, linestyle='-')\n",
    "    plt.plot(layers_to_analyze, entropies_nolbl[lang], marker='o', label=f\"{lang.upper()} (No LBL)\", color=color, linestyle='--', alpha=0.7)\n",
    "    plt.plot(layers_to_analyze, entropies_lbl[lang], marker='o', label=f\"{lang.upper()} (LBL)\", color=color, linestyle=':', alpha=0.7)\n",
    "plt.title(\"Entropy across Layers\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Put legend outside\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')\n",
    "plt.savefig(\"analysis/entropy.png\", bbox_inches='tight')\n",
    "print(\"Entropy across Layers saved at analysis/entropy.png\")\n",
    "\n",
    "\n",
    "# --- Plot KL Divergence ---\n",
    "plt.figure(figsize=(10, 4))\n",
    "for lang in languages:\n",
    "    color = color_map.get(lang, 'black')\n",
    "    plt.plot(layers_to_analyze, kls[lang], marker='o', label=lang.upper(), color=color, linestyle='-')\n",
    "    plt.plot(layers_to_analyze, kls_nolbl[lang], marker='o', label=f\"{lang.upper()} (No LBL)\", color=color, linestyle='--', alpha=0.7)\n",
    "    plt.plot(layers_to_analyze, kls_lbl[lang], marker='o', label=f\"{lang.upper()} (LBL)\", color=color, linestyle=':', alpha=0.7)\n",
    "plt.title(\"KL Divergence from Uniform across Layers\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"KL Divergence\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')\n",
    "plt.savefig(\"analysis/kl_divergence.png\", bbox_inches='tight')\n",
    "print(\"KL Divergence from Uniform across Layers saved at analysis/kl_divergence.png\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
